# Baseline решение для кейса MCMOT

Задача: multi-camera multi-object tracking (MCMOT).

## Задача

В рамках хакатона требуется разработать алгоритм трекинга людей по нескольким камерам. Алгоритм должен уметь:

 - Восстанавливать треки при кратковременных перекрытиях.
 - Реидентифицировать людей между камерами, которые будут установлены как в одном помещении, так и в нескольких смежных.

Ассоциация треков между кадрами должна осуществляться по различным косвенным признакам:
 
 - Одежда: разделение на два трека одного человека в разной одежде не считается ошибкой. Однако будет плюсом, если алгоритм сможет определить, что человек сменил одежду между сценами.
 - Походка.
 - Пол, возраст, рост, телосложение и другие характеристики.

При этом исключается явное распознавание лиц, чтобы обеспечить работу алгоритма в сложных сценариях: толпа, частичное перекрытие, съемка со спины и т.д.

Алгоритм должен работать в реальном времени при одновременной обработке видеопотоков с нескольких камер на одном устройстве. Он должен быть оформлен в виде компьютерной программы, скрипта или jupyter ноутбука.

Входные данные — видеоролики, записанные одновременно с разных камер; результат работы — детекции людей в текстовом формате, обозначенные прямоугольниками для каждого кадра, и их уникальные идентификаторы (ID). Будет плюсом, если результат работы будет визуализирован.
 - Ко второму чек-поинту должен быть реализован алгоритм реидентификации людей с разных камер.
 - К третьему чек-поинту алгоритм реидентификации должен быть встроен в алгоритм трекинга.

Обязательно подготовить ваше решения для тестирования на закрытом наборе, как указано ниже.

## Создание виртуального окружения

```
conda create python=3.8 --name venv-baseline
conda activate venv-baseline
pip install -r requirements-env.txt
```

Файл `requirements.txt` содерджит в себе зависимости, необходимые для установки в docker-образе для инференса (см. `Dockerfile`).


## Модель для ре-идентификации людей на разных камерах

Скрипты для обучения и датасеты были взяты из [этого](https://github.com/chenhao2345/UCR) репозитория. Итоговая модель была обучена на датасетах Market1501, CUHK03, PRID в supervised-режиме.


## Инференс модели на видео с нескольких камер

1. Вне контейнера:
```
PYTHONPATH=./ python inference.py --videos_dir <video_dir> -m assets/ -s results/
```

2. В контейнере для инференса:

```

docker build . -t reid_pytorch

docker run --gpus 0 -v $(pwd):/workspace/ -v {private_test_data}:/private_test/:ro -w /workspace/ reid_pytorch sh inference.sh

```

Расположение файлов внутри докер-контейнера:

```
├── private_test/ 
│ └──  videos/*.mp4
├── workspace/
│ ├── inference.sh
│ ├── ...
│ └── results/
```

**Перед отправкой своего решения на проверку необходимо убедиться, что команда `docker run ...` для получения предсказаний работает корректно в docker-контейнере.**


## Валидация

Для валидации решений используется репозиторий [TrackEval](https://github.com/JonathonLuiten/TrackEval), метрика HOTA. Чтобы провалидировать свое решение на публичном тестовом датасете, необходимо:

1. Склонировать репозиторий `TrackEval`;

2. Предсказания модели объединить в один файл (формат предсказаний см. в "Инструкция для участников"):

```
python scripts/merge_predictions_for_eval.py -p <path/to/directory/with/model_txt_predictions> -v <path/to/directory/with/test_videos>
```

3. Организовать файлы следующим образом:

```
TrackEval/
|    data/
|    | - gt/
|    |  | - mot_challenge/
|    |  |  | - ISS_HACK_REID-test/
|    |  |  |  | - PublicTest/
|    |  |  |  |  | - gt/
|    |  |  |  |  |  | - gt.txt
|    |  |  |  |  | - seqinfo.ini
|    |  |  | - seqmaps/
|    |  |  |  | - ISS_HACK_REID-test.txt
|
|    | - trackers/
|    |  | - mot_challenge/
|    |  |  | - ISS_HACK_REID-test/
|    |  |  |  | - solution/
|    |  |  |  |  | - PublicTest.txt
```

где файлы `gt.txt`, `seqinfo.ini`, `ISS_HACK_REID-test.txt` предоставляются организаторами, а в файле `PublicTest.txt` должны храниться предсказания модели, полученные в п.2.

4. Выполнить команду:

```
cd TrackEval
python scripts/run_mot_challenge.py --BENCHMARK ISS_HACK_REID --DO_PREPROC False --METRICS HOTA --SPLIT_TO_EVAL test --TRACKER_SUB_FOLDER ./
```

## Инструкция для участников

Конфигурация тестовой системы:

- CPU Intel Core i7-14700K;
- RAM: 32 Gb;
- GPU: NVIDIA GeForce RTX 4060 Ti 16Gb.

Все видео, которые входят в тестовый датасет, имеют расширение `mp4`.

 Участники должны предоставить репозиторий своего решения, который должен содержать:

1) веса обученных моделей;
2) файл `requirements.txt`, если они используют специфические зависимости;
3) bash скрипт для инференса их пайплайна ре-идентификации (`inference.sh`). Этот скрипт должен на вход принимать произвольное количество (N) видео, на выходе - генерировать N файлов предсказаний модели в **следующем формате**:

```
<frame_id>, <object_id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <conf>, 1, 1, 1
```

где:

- `frame_id` - порядковый номер кадра (начинается с 1);
- `object_id` - уникальный ID объекта (трека). ID объектов должны сохраняться одинаковыми для одних и тех же объектов на разных видео.
- `<bb_left>, <bb_top>, <bb_width>, <bb_height>` - координаты bbox-а объекта;
- `conf` - уверенность предсказания.

Эти файлы должны сохраняться в отдельную папку.

**Пример:** на вход подаются файлы `test_video1.mp4`, `test_video2.mp4`. Скрипт должен выдать файлы `test_video1.txt`, `test_video2.txt`. 

В качестве референса следует ориентироваться на файлы `inference.sh` и `inference.py`.  Аргументы командной строки должны совпадать с аргументами из файла `inference.py`, где:

- `--videos_dir` - путь до папки, в котором лежат видео для инференса. При получении предсказаний на закрытом тесте это  путь `/private_test/`;

- `--mount` -  путь до папки, в котором лежат необходимые файлы (в т.ч. веса моделей);

- `--save_dir` -  путь до папки, в которую будут записываться предсказания модели.
